import shutil
import time
import warnings

from pathlib import Path

import click
import numpy as np
import pandas as pd
import submitit

from iohub import open_ome_zarr
from iohub.ngff import Plate

from biahub.analysis.AnalysisSettings import StitchSettings
from biahub.analysis.stitch import (
    get_grid_rows_cols,
    get_image_shift,
    get_stitch_output_shape,
    preprocess_and_shift,
    stitch_shifted_store,
)
from biahub.cli.parsing import config_filepath, input_position_dirpaths, output_dirpath
from biahub.cli.utils import create_empty_hcs_zarr, process_single_position_v2, yaml_to_model

HAS_SLURM = True

@click.command()
@input_position_dirpaths()
@output_dirpath()
@config_filepath()
@click.option(
    "--temp-path",
    type=click.Path(exists=True, file_okay=False, dir_okay=True),
    default='./',
    help="Path to temporary directory, ideally with fast read/write speeds, e.g. /hpc/scratch/group.comp.micro/",
)
@click.option("--debug", is_flag=True, help="Run in debug mode")
def stitch(
    input_position_dirpaths: list[Path],
    output_dirpath: str,
    config_filepath: str,
    temp_path: str,
    debug: bool,
) -> None:
    """
    Stitch positions in wells of a zarr store using a configuration file generated by estimate-stitch.

    >>> biahub stitch -i ./input.zarr/*/*/* -c ./stitch_params.yml -o ./output.zarr --temp-path /hpc/scratch/group.comp.micro/
    """
    if not HAS_SLURM:
        warnings.warn(
            "This function is intended to be used with SLURM. "
            "Running on local machine instead."
        )
    # assert not Path(output_dirpath).exists(), f'Output path: {output_dirpath} already exists'

    slurm_out_path = Path(output_dirpath).parent / "slurm_output"
    dataset = input_position_dirpaths[0].parts[-4][:-5]
    well0 = '_'.join(input_position_dirpaths[0].parts[-3:-1])
    shifted_store_path = Path(
        temp_path,
        f"TEMP_{dataset}_{well0}.zarr"
    )
    settings = yaml_to_model(config_filepath, StitchSettings)

    with open_ome_zarr(str(input_position_dirpaths[0]), mode="r") as input_dataset:
        input_dataset_channels = input_dataset.channel_names
        T, C, Z, Y, X = input_dataset.data.shape
        scale = tuple(input_dataset.scale)
        chunks = input_dataset.data.chunks

    if settings.channels is None:
        settings.channels = input_dataset_channels

    assert all(
        channel in input_dataset_channels for channel in settings.channels
    ), "Invalid channel(s) provided."

    position_paths = [Path(*p.parts[-3:]).as_posix() for p in input_position_dirpaths]
    wells = list(set([Path(*p.parts[-3:-1]).as_posix() for p in input_position_dirpaths]))
    fov_names = set([p.name for p in input_position_dirpaths])
    grid_rows, grid_cols = get_grid_rows_cols(fov_names)
    n_rows = len(grid_rows)
    n_cols = len(grid_cols)

    if all((settings.column_translation, settings.row_translation)):
        output_shape, global_translation = get_stitch_output_shape(
            n_rows, n_cols, Y, X, settings.column_translation, settings.row_translation
        )
    elif settings.total_translation is not None:
        all_shifts = []
        for _pos, _shift in settings.total_translation.items():
            if _pos in position_paths:
                all_shifts.append(_shift)
        output_shape = np.ceil(np.asarray(all_shifts).max(axis=0) + np.asarray([Y, X]))
        output_shape = tuple(output_shape.astype(int))
    elif settings.affine_transform is not None:
        all_shifts = []
        for _pos, _transform in settings.affine_transform.items():
            if _pos in position_paths:
                # These are inverse transforms so here we take the negative shift
                all_shifts.append([-_transform[1][3], -_transform[2][3]])
        output_shape = np.ceil(np.asarray(all_shifts).max(axis=0) - np.asarray(all_shifts).min(axis=0) + np.asarray([Y, X]))
        output_shape = tuple(output_shape.astype(int))
    else:
        raise ValueError('Invalid RegistrationSettings config file')

    # create output zarr store
    stitched_shape = (T, len(settings.channels), Z) + output_shape
    stitched_chunks = chunks[:3] + (4096, 4096)
    create_empty_hcs_zarr(
        store_path=output_dirpath,
        position_keys=[Path(well, '0').parts for well in wells],
        channel_names=settings.channels,
        shape=stitched_shape,
        chunks=stitched_chunks,
        scale=scale,
    )

    shift_job_ids = []
    if shifted_store_path.exists():
        click.echo(f'WARNING: Using existing shifted zarr store at {shifted_store_path}')
    else:
        # create temp zarr store
        click.echo(f'Creating temporary zarr store at {shifted_store_path}')
        with open_ome_zarr(
            Path(temp_path, f'temp_{well0}.zarr'), layout='hcs', mode='w', channel_names=settings.channels
        ) as temp_zarr:
            pos = temp_zarr.create_position(*input_position_dirpaths[0].parts[-3:])
            pos.create_zeros(
                name='0',
                shape=stitched_shape,
                chunks=stitched_chunks,
                dtype=np.float32,
            )

        slurm_args = {
            "slurm_mem_per_cpu": "8G",
            "slurm_cpus_per_task": 1,
            "slurm_time": 30,
            "slurm_job_name": "temp_store",
            "slurm_partition": "cpu",
        }

        executor = submitit.AutoExecutor(folder=slurm_out_path)
        executor.update_parameters(**slurm_args)
        temp_zarr_job = executor.submit(
            Plate.from_positions,
            shifted_store_path,
            dict((Path(*p.parts[-3:]).as_posix(), pos) for p in input_position_dirpaths)
        )

        # Collect transforms
        transforms = []
        for in_path in input_position_dirpaths:
            well = Path(*in_path.parts[-3:-1])
            col, row = (in_path.name[:3], in_path.name[3:])
            fov = str(well / (col + row))

            if settings.affine_transform is not None:
                # COL+ROW order here is important
                transforms.append(settings.affine_transform[fov])
            elif settings.total_translation is not None:
                transforms.append(settings.total_translation[fov])
            else:
                transforms.append(
                    get_image_shift(
                        int(col),
                        int(row),
                        settings.column_translation,
                        settings.row_translation,
                        global_translation,
                    )
                )

        slurm_args = {
            "slurm_mem_per_cpu": "24G",
            "slurm_cpus_per_task": 6,
            "slurm_array_parallelism": 100,  # only 100 jobs can run at the same time
            "slurm_time": 30,
            "slurm_job_name": "shift",
            "slurm_partition": "cpu",
            "slurm_dependency": f"afterok:{temp_zarr_job.job_id}",
        }
        # Affine transform needs more resources
        if settings.affine_transform is not None:
            slurm_args.update(
                {
                    "slurm_mem_per_cpu": "48G",
                    "slurm_cpus_per_task": 8,
                    "slurm_time": 60,
                }
            )

        executor = submitit.AutoExecutor(folder=slurm_out_path)
        executor.update_parameters(**slurm_args)
        click.echo('Submitting SLURM jobs')
        shift_jobs = []
        with executor.batch():
            for in_path, transform in zip(input_position_dirpaths, transforms):
                job = executor.submit(
                    process_single_position_v2,
                    preprocess_and_shift,
                    input_channel_idx=[
                        input_dataset_channels.index(ch) for ch in settings.channels
                    ],
                    output_channel_idx=list(range(len(settings.channels))),
                    time_indices='all',
                    num_processes=slurm_args['slurm_cpus_per_task'],
                    settings=settings.preprocessing,
                    output_shape=output_shape,
                    verbose=True,
                    transform=transform,
                    input_data_path=in_path,
                    output_path=shifted_store_path,
                )
                shift_jobs.append(job)

        shift_job_ids = [job.job_id for job in shift_jobs]

    slurm_args = {
        "slurm_mem_per_cpu": "8G",
        "slurm_cpus_per_task": 64,
        "slurm_time": "3-00:00:00",  # in [DD-]HH:MM:SS format
        "slurm_partition": "cpu",
        "slurm_job_name": "stitch",
    }
    if shift_job_ids:
        slurm_args["slurm_dependency"] = f"afterok:{shift_job_ids[0]}:{shift_job_ids[-1]}"


    executor = submitit.AutoExecutor(folder=slurm_out_path)
    executor.update_parameters(**slurm_args)

    stitch_job = executor.submit(
        stitch_shifted_store,
        shifted_store_path,
        output_dirpath,
        settings.postprocessing,
        blending='average',
        verbose=True,
    )

    if not debug:
        slurm_args = {
            "slurm_partition": "cpu",
            "slurm_mem_per_cpu": "12G",
            "slurm_cpus_per_task": 1,
            "slurm_time": "0-01:00:00",  # in [DD-]HH:MM:SS format
            "slurm_job_name": "cleanup",
            "slurm_dependency": f"afterok:{stitch_job.job_id}",
        }
        executor = submitit.AutoExecutor(folder=slurm_out_path)
        executor.update_parameters(**slurm_args)
        executor.submit(shutil.rmtree, shifted_store_path)


if __name__ == '__main__':
    stitch()
